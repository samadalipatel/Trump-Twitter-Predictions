{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "# For scraping\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "# Twitter scraping\n",
    "import tweepy \n",
    "import json\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "# For removing and counting stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# For lemmatization \n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "# For idf and tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "class TrumpTwitterPredictions: \n",
    "    \n",
    "    # Initialize class by loading the model \n",
    "    def __init__(self, model_file):\n",
    "        with open(model_file, 'rb') as f: \n",
    "            self.model = pickle.load(f)\n",
    "    \n",
    "    # Collects data from trackanalytics and twitter - requires Twitter API credentials \n",
    "    def CollectData(self, consumer_key, consumer_secret, access_key, access_secret, num_tweets):\n",
    "        ####################################\n",
    "        # Collect and Clean Followers Data #\n",
    "        ####################################\n",
    "        \n",
    "        ### COLLECT ###\n",
    "        # Create and open url \n",
    "        url = \"https://www.trackalytics.com/twitter/profile/realdonaldtrump/\"\n",
    "        html = urlopen(url)\n",
    "\n",
    "        # Create BS object \n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Find table on this page - only one table, index of 0\n",
    "        table = soup.findAll('table')[0]\n",
    "\n",
    "        # Get column names (exist in second row of table)\n",
    "        colnames = [th.getText() for th in table.findAll('tr', limit=2)[1].findAll('th')]\n",
    "\n",
    "        # Collect every row with non-header data \n",
    "        data_rows = table.findAll('tr')[2:] \n",
    "\n",
    "        # For every row, create a list of column data, and append those lists into one large list \n",
    "        follower_data = [[td.getText() for td in data_rows[i].findAll('td')] for i in range(len(data_rows))]\n",
    "\n",
    "        # Create dataframe \n",
    "        df = pd.DataFrame(follower_data, columns=colnames, dtype=str)\n",
    "        \n",
    "        ### CLEAN ###\n",
    "        # Remove first column\n",
    "        df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "        # Extract number of followers for each date \n",
    "        followers = df['Followers  (change)'].apply(lambda x: x.split(' ')[0])\n",
    "        # Remove commas and change to integer\n",
    "        followers = followers.str.replace(',', '').astype(int)\n",
    "\n",
    "        # Extract change for each date\n",
    "        follower_change = df['Followers  (change)'].apply(lambda x: x.split(' ')[2])\n",
    "        # Remove parentheses, pluses, minuses, and commas and change to integer\n",
    "        follower_change = follower_change.str.replace('\\\\(|\\\\)|\\\\+|\\\\-|\\\\,', '').astype(int)\n",
    "\n",
    "        # Extract number of tweets per day\n",
    "        tweets_that_day = df['Tweets  (change)'].apply(lambda x: x.split(' ')[2])\n",
    "        # Remove parentheses, pluses, minuses, and commas and change to integer\n",
    "        tweets_that_day = tweets_that_day.str.replace('\\\\(|\\\\)|\\\\+|\\\\-|\\\\,', '').astype(int)\n",
    "\n",
    "        # Format date\n",
    "        date = pd.to_datetime(df.Date)\n",
    "\n",
    "        # Place these features into their own dataframe \n",
    "        FollowersData = pd.DataFrame({'Date': date, 'Followers': followers, 'Follower_Change': follower_change, \n",
    "                                      'Num_Tweets': tweets_that_day})\n",
    "        # Change type of Date to integer for merging purposes\n",
    "        FollowersData.Date = FollowersData.Date.astype(str)\n",
    "        \n",
    "        ##################################\n",
    "        # Collect and Clean Twitter Data #\n",
    "        ##################################\n",
    "        \n",
    "        ### COLLECT ###\n",
    "        # Twitter API credentials\n",
    "        con_key = consumer_key\n",
    "        con_secret = consumer_secret\n",
    "        acc_key = access_key\n",
    "        acc_secret = access_secret \n",
    "        \n",
    "        # Authorize twitter, initialize tweepy\n",
    "        auth = tweepy.OAuthHandler(con_key, con_secret)\n",
    "        auth.set_access_token(acc_key, acc_secret)\n",
    "        api = tweepy.API(auth)\n",
    "\n",
    "        # Collect tweets\n",
    "        tweets = api.user_timeline(screen_name = 'realDonaldTrump', count = num_tweets,\n",
    "                                   tweet_mode = \"extended\", include_rts = False)\n",
    "\n",
    "        # Create dataframe with tweet text, date, and favorite count\n",
    "        TweetsData = pd.DataFrame([[tweet.created_at, tweet.full_text, tweet.favorite_count] for tweet in tweets], \n",
    "                         columns = ['created_at', 'text', 'favorite_count'])\n",
    "        \n",
    "        ### CLEAN ###\n",
    "        # Format Date\n",
    "        TweetsData['Date'] = TweetsData.created_at.dt.strftime('%Y-%m-%d')\n",
    "        # Change type of Date to integer for merging purposes\n",
    "        TweetsData.Date = TweetsData.Date.astype(str)\n",
    "        \n",
    "        \n",
    "        #########\n",
    "        # MERGE #\n",
    "        #########\n",
    "        # Merge the two dataframes, return \n",
    "        cleaned_df = pd.merge(TweetsData, FollowersData, on='Date', how = 'left')\n",
    "        return(cleaned_df)\n",
    "    \n",
    "    \n",
    "    ### Helper functions for feature engineering ###\n",
    "    # Input: Tweet. Output: Average word length of tweet. \n",
    "    def avg_word_length(self, tweet): \n",
    "        words = tweet.split()\n",
    "        if len(words) > 0:\n",
    "            return(sum(len(word) for word in words) / len(words))\n",
    "        else: \n",
    "            return(0)\n",
    "        \n",
    "    # Input: Tweet. Output: Average term-frequency rate     \n",
    "    def avg_tf_rate(self, tweet):\n",
    "        # For empty tweets (tweets that were likely just urls), mean tf = 0\n",
    "        if len(tweet) == 0:\n",
    "            return(0)\n",
    "        # For actual tweets, calculate it out \n",
    "        else: \n",
    "            return(np.mean(pd.value_counts(tweet.split(' '))/len(tweet)))\n",
    "    \n",
    "    # Input: tweet, idf_df. Output: Average inverse-document-frequency rate \n",
    "    def avg_idf_rate(self, tweet, df):\n",
    "        # Tokenize the tweet\n",
    "        tokenized_tweet = tweet.split(' ')\n",
    "        # Find the idf per word using the idf_df \n",
    "        idf_df = df\n",
    "        idf = idf_df[idf_df.Phrase.isin(tokenized_tweet)].IDF.values\n",
    "        # Safeguard against empty lists - that means idf = 0\n",
    "        if len(idf) == 0:\n",
    "            return(0)\n",
    "        else: \n",
    "            # Otherwise, return the average \n",
    "            return(np.mean(idf))\n",
    "\n",
    "\n",
    "    def EngineerFeatures(self, df): \n",
    "        ###################\n",
    "        # Basic Features #\n",
    "        ##################\n",
    "        \n",
    "        # Create Year, Month, Week, Day, Hour variables \n",
    "        df['Year'] = df.created_at.dt.year\n",
    "        df['Month'] = df.created_at.dt.month\n",
    "        df['Week'] = df.created_at.dt.week\n",
    "        df['Day'] = df.created_at.dt.day\n",
    "        df['Hour'] = df.created_at.dt.hour\n",
    "        \n",
    "        # Add holidays feature from full_holidays.csv - read and merge \n",
    "        fullholidays = pd.read_csv('fullholidays.csv')\n",
    "        df = pd.merge(df, fullholidays, on = 'Date', how = 'left')\n",
    "        # Any non-holiday gets 0 - it will show up as null\n",
    "        df.iloc[df.Holiday.isnull().index, np.where(df.columns.isin(['Holiday']))[0]] = 0\n",
    "        \n",
    "        # Remove urls\n",
    "        df['trump_text'] = df.text.str.replace(r'http\\S+', '')\n",
    "        \n",
    "        # Get word count \n",
    "        df['Word_Count'] = df.trump_text.apply(lambda x: len(str(x).split(' ')))\n",
    "        \n",
    "        # Presence of URLs\n",
    "        # Create column with all 0s\n",
    "        df['Any_urls'] = 0\n",
    "        # Find indices with 0s\n",
    "        indices_with_urls = df.text.str.extractall(r'(http\\S+)').index.get_level_values(0)\n",
    "        # Make those indices 1\n",
    "        df.iloc[indices_with_urls, np.where(df.columns.isin(['Any_urls']))[0]] = 1\n",
    "\n",
    "        # Character count \n",
    "        df['Character_Count'] = df.trump_text.str.len()\n",
    "        \n",
    "        # Average word length\n",
    "        df['avg_word_len'] = df.trump_text.apply(self.avg_word_length)\n",
    "        \n",
    "        # Count number of stopwords \n",
    "        swords = stopwords.words('english')\n",
    "        df['Num_Stopwords'] = df.trump_text.apply(lambda x: \n",
    "                                                  len([word for word in x.split() if word.lower() in swords]))\n",
    "        \n",
    "        # Presence of Hashtags\n",
    "        # Presence \n",
    "        indices_with_hashtags = df.trump_text.str.extractall(r'(#)', re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create an indicator variable, set all to 0 \n",
    "        df['Any_Hashtags'] = 0\n",
    "        # Set indices_with_hashtags to 1\n",
    "        df.iloc[indices_with_hashtags, np.where(df.columns.isin(['Any_Hashtags']))[0]] = 1\n",
    "\n",
    "        # Number of Hashtags\n",
    "        df['Num_Hashtags'] = df.trump_text.apply(lambda x: \n",
    "                                                 len([word for word in x.split() if word.startswith('#')]))\n",
    "        \n",
    "        # Presence of Mentions\n",
    "        indices_with_mentions = df.trump_text.str.extractall(r'(@)', re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Mentions'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_mentions, np.where(df.columns.isin(['Any_Mentions']))[0]] = 1\n",
    "\n",
    "        # Number of Mentions\n",
    "        df['Num_Mentions'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.startswith('#')]))\n",
    "        \n",
    "        # Number of Uppercase words Per tweet \n",
    "        df['Num_Upper'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.isupper()]))\n",
    "\n",
    "        # Presence of Fully Uppercase Tweets \n",
    "        df['Upper'] = df.trump_text.str.isupper().astype(int)\n",
    "\n",
    "        # Number of Exclamation Points\n",
    "        df['Num_Exclaim'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.endswith('!')]))\n",
    "\n",
    "        \n",
    "        # Presence of Clintons \n",
    "        indices_with_clintons = df.trump_text.str.extractall(r'(hillary)|(hillary clinton)|(clinton)|(clintons)|(bill clinton)', \n",
    "                                                     re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Clinton'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_clintons, np.where(df.columns.isin(['Any_Clinton']))[0]] = 1\n",
    "        \n",
    "        \n",
    "        # Presence of Obama\n",
    "        indices_with_obama = df.trump_text.str.extractall(r'(obama)|(barrack)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Obama'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_obama, np.where(df.columns.isin(['Any_Obama']))[0]] = 1\n",
    "\n",
    "        \n",
    "        # Presence of MAGA\n",
    "        indices_with_maga = df.trump_text.str.extractall(r'(MAKE AMERICA GREAT AGAIN)|(MAGA)|(#MAKEAMERICAGREATAGAIN)|(#MAGA)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_MAGA'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_maga, np.where(df.columns.isin(['Any_MAGA']))[0]] = 1       \n",
    "        \n",
    "        \n",
    "        # Presence of Sad/Bad\n",
    "        indices_with_ad = df.trump_text.str.extractall(r'(sad)|(bad)|(sad!)|(bad!)|(sad.)|(bad.)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_ad'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_ad, np.where(df.columns.isin(['Any_ad']))[0]] = 1\n",
    "        \n",
    "        \n",
    "        # Presence of Democrats\n",
    "        indices_with_dems = df.trump_text.str.extractall(r'(democrat)|(democratic)|(democrats)|(dems)|(dem)|(dnc)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Dems'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_dems, np.where(df.columns.isin(['Any_Dems']))[0]] = 1\n",
    "        \n",
    "        \n",
    "        # Presence of Republicans\n",
    "        indices_with_reps = df.trump_text.str.extractall(r'(republican)|(rep)|(gop)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Rep'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_reps, np.where(df.columns.isin(['Any_Rep']))[0]] = 1\n",
    "\n",
    "\n",
    "        # Presence of fake news \n",
    "        indices_with_media = df.trump_text.str.extractall(r'(fakenews)|(fake news)|(media)|(fake)|(fakenews!)', \n",
    "                                                          re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create column, set all = 0\n",
    "        df['FakeNews'] = 0\n",
    "        # Set those with mentions of fake news = 1\n",
    "        df.iloc[indices_with_media, np.where(df.columns.isin(['FakeNews']))[0]] = 1\n",
    "\n",
    "\n",
    "        # Presence of lies\n",
    "        indices_mentioning_lies = df.trump_text.str.extractall(r'(liar)|(lie)|(false)|(faker)|(con)|(hoax)',\n",
    "                                                              re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create column\n",
    "        df['Any_Lie_Mentions'] = 0\n",
    "        # Make indices_mentioning_lies = 1\n",
    "        df.iloc[indices_mentioning_lies, np.where(df.columns.isin(['Any_Lie_Mentions']))[0]] = 1\n",
    "\n",
    "\n",
    "        # Presence of immigration\n",
    "        indices_mentioning_immigrants = df.trump_text.str.extractall(r'(immigration)|(immigrants)|(border)|(wall)|(buildthewall)|(wall)|(thewall)|(caravan)',\n",
    "                                                              re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create column, set all = 0\n",
    "        df['Any_Immigration'] = 0\n",
    "        # Set those with mentions of fake news = 1\n",
    "        df.iloc[indices_mentioning_immigrants, np.where(df.columns.isin(['Any_Immigration']))[0]] = 1\n",
    "\n",
    "        ####################\n",
    "        # Preprocess text #\n",
    "        ###################\n",
    "        # Lowercase\n",
    "        df.trump_text = df.trump_text.str.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        df.trump_text = df.trump_text.str.replace(pat = '[^\\w\\s]', repl = '')\n",
    "\n",
    "        # Remove stopwords\n",
    "        df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in swords]))\n",
    "\n",
    "        # Remove most commonly occuring words \n",
    "        freq_words = list(pd.Series(' '.join(df['trump_text']).split()).value_counts()[:10].index)\n",
    "        df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in freq_words]))\n",
    "\n",
    "        # Remove all words that were only used once \n",
    "        infreq_words = pd.Series(' '.join(df['trump_text']).split()).value_counts()\n",
    "        infreq_words = infreq_words[infreq_words.values == 1]\n",
    "        infreq_words = list(infreq_words.index)\n",
    "        df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in infreq_words]))\n",
    "\n",
    "        # Lemmatize\n",
    "        df.trump_text = df.trump_text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "        # Algorithmic Text Processing #\n",
    "        ###############################\n",
    "        # TF\n",
    "        df['tf'] = df.trump_text.apply(lambda x: self.avg_tf_rate(x))\n",
    "        \n",
    "        # IDF\n",
    "        # Create tfidf vectorizer\n",
    "        vectorizer = TfidfVectorizer(lowercase=True, analyzer='word', ngram_range=(1,1))\n",
    "        # Load the training text and replace nulls to be empty \n",
    "        training_text = pd.read_csv('training_text.csv')\n",
    "        training_text[training_text.isnull()] = ''\n",
    "        # Append training text to test text \n",
    "        full_text = training_text.trump_text.append(df.trump_text, ignore_index=True)\n",
    "        # Fit the vocabulary \n",
    "        idf_fit = vectorizer.fit(full_text)\n",
    "        # Use the fit to find idf per phrase - save into a dataframe \n",
    "        idf_df = pd.DataFrame({'Phrase': idf_fit.get_feature_names(), 'IDF': idf_fit.idf_})\n",
    "        # Create column\n",
    "        df['idf'] = df.trump_text.apply(lambda x: self.avg_idf_rate(x, idf_df))\n",
    "        \n",
    "        # TF-IDF\n",
    "        df['tfidf'] = df.tf * df.idf\n",
    "        \n",
    "        # Sentiment\n",
    "        df['Sentiment'] = df.trump_text.apply(lambda x: TextBlob(x).sentiment[0])\n",
    "\n",
    "        # Subjectivity \n",
    "        df['Subjectivity'] = df.trump_text.apply(lambda x: TextBlob(x).sentiment[1])\n",
    "        \n",
    "        # Remove irrelevant columns\n",
    "        df.drop(columns=['trump_text', 'text', 'created_at', 'Date'], axis = 1, inplace = True)\n",
    "        \n",
    "        # In case program is run before trackanalytics has updated follower info...\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # Split \n",
    "        X = df.drop(columns=['favorite_count'], axis = 1)\n",
    "        y = df.favorite_count\n",
    "\n",
    "        return(X, y)\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = self.model.predict(X)\n",
    "        return(pred)\n",
    "    \n",
    "    def GenerateReport(self, y_pred, y_true): \n",
    "        report_df = pd.DataFrame({'Predictions': np.ceil(y_pred), 'Actual': y_true, \n",
    "                                  'Absolute Error': np.abs(np.round(y_pred) - y_true),\n",
    "                                  'Percent Error': np.round((np.abs((y_pred - y_true)/y_true))*100)})\n",
    "        return(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = TrumpTwitterPredictions('lasso_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mod.CollectData(consumer_key = \"xK8iqfsNIOJCA4sXDQVxcmFKq\",\n",
    "consumer_secret = \"Lt5dBPX0SUWa9TskVUlydhilBqhu4JcHy13jg1IwoM6V672MkY\",\n",
    "access_key = \"1013962412091850754-L1fvfqKUEaSfrLcRSwmZvhlB79aRcw\",\n",
    "access_secret = \"HN35iHf3wAcFeczjIrDjjjdsmMvBkPRAkSK9VQG7rutht\",\n",
    "num_tweets = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mod.EngineerFeatures(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (16,35) (37,) (16,35) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bf02affb463e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-3ea6b226be70>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (16,35) (37,) (16,35) "
     ]
    }
   ],
   "source": [
    "pred = mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Absolute Error</th>\n",
       "      <th>Percent Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58235.0</td>\n",
       "      <td>52207</td>\n",
       "      <td>6027.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676.0</td>\n",
       "      <td>48364</td>\n",
       "      <td>3312.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57650.0</td>\n",
       "      <td>67334</td>\n",
       "      <td>9684.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82807.0</td>\n",
       "      <td>134331</td>\n",
       "      <td>51524.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82473.0</td>\n",
       "      <td>74316</td>\n",
       "      <td>8157.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predictions  Actual  Absolute Error  Percent Error\n",
       "0      58235.0   52207          6027.0           12.0\n",
       "1      51676.0   48364          3312.0            7.0\n",
       "2      57650.0   67334          9684.0           14.0\n",
       "3      82807.0  134331         51524.0           38.0\n",
       "4      82473.0   74316          8157.0           11.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df = mod.GenerateReport(pred, y)\n",
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_csv(index=False, header = True, path_or_buf='PredictionReport.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(path_or_buf='X_test.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv(path='y_test.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
