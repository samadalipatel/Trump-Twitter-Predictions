{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "# For scraping\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "# Twitter scraping\n",
    "import tweepy \n",
    "import json\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "# For removing and counting stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# For lemmatization \n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "# For idf and tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-246-2c7963c757bb>, line 117)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-246-2c7963c757bb>\"\u001b[0;36m, line \u001b[0;32m117\u001b[0m\n\u001b[0;31m    if len(tweet) == 0:\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class TrumpTwitterPredictions: \n",
    "    \n",
    "    # Initialize class by loading the model \n",
    "    def __init__(self, model_file):\n",
    "        with open(model_file, 'rb') as f: \n",
    "            self.model = pickle.load(f)\n",
    "    \n",
    "    # Collects data from trackanalytics and twitter - requires Twitter API credentials \n",
    "    def CollectData(self, consumer_key, consumer_secret, access_key, access_secret):\n",
    "        ####################################\n",
    "        # Collect and Clean Followers Data #\n",
    "        ####################################\n",
    "        \n",
    "        ### COLLECT ###\n",
    "        # Create and open url \n",
    "        url = \"https://www.trackalytics.com/twitter/profile/realdonaldtrump/\"\n",
    "        html = urlopen(url)\n",
    "\n",
    "        # Create BS object \n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # Find table on this page - only one table, index of 0\n",
    "        table = soup.findAll('table')[0]\n",
    "\n",
    "        # Get column names (exist in second row of table)\n",
    "        colnames = [th.getText() for th in table.findAll('tr', limit=2)[1].findAll('th')]\n",
    "\n",
    "        # Collect every row with non-header data \n",
    "        data_rows = table.findAll('tr')[2:] \n",
    "\n",
    "        # For every row, create a list of column data, and append those lists into one large list \n",
    "        follower_data = [[td.getText() for td in data_rows[i].findAll('td')] for i in range(len(data_rows))]\n",
    "\n",
    "        # Create dataframe \n",
    "        df = pd.DataFrame(follower_data, columns=colnames, dtype=str)\n",
    "        \n",
    "        ### CLEAN ###\n",
    "        # Remove first column\n",
    "        df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "        # Extract number of followers for each date \n",
    "        followers = df['Followers  (change)'].apply(lambda x: x.split(' ')[0])\n",
    "        # Remove commas and change to integer\n",
    "        followers = followers.str.replace(',', '').astype(int)\n",
    "\n",
    "        # Extract change for each date\n",
    "        follower_change = df['Followers  (change)'].apply(lambda x: x.split(' ')[2])\n",
    "        # Remove parentheses, pluses, minuses, and commas and change to integer\n",
    "        follower_change = follower_change.str.replace('\\\\(|\\\\)|\\\\+|\\\\-|\\\\,', '').astype(int)\n",
    "\n",
    "        # Extract number of tweets per day\n",
    "        tweets_that_day = df['Tweets  (change)'].apply(lambda x: x.split(' ')[2])\n",
    "        # Remove parentheses, pluses, minuses, and commas and change to integer\n",
    "        tweets_that_day = tweets_that_day.str.replace('\\\\(|\\\\)|\\\\+|\\\\-|\\\\,', '').astype(int)\n",
    "\n",
    "        # Format date\n",
    "        date = pd.to_datetime(df.Date)\n",
    "        # Change type of Date to integer for merging purposes\n",
    "        FollowersData.Date = FollowersData.Date.astype(str)\n",
    "\n",
    "        # Place these features into their own dataframe \n",
    "        FollowersData = pd.DataFrame({'Date': date, 'Followers': followers, 'Follower_Change': follower_change, \n",
    "                                      'Num_Tweets': tweets_that_day})\n",
    "        \n",
    "        \n",
    "        ##################################\n",
    "        # Collect and Clean Twitter Data #\n",
    "        ##################################\n",
    "        \n",
    "        ### COLLECT ###\n",
    "        # Twitter API credentials\n",
    "        con_key = consumer_key\n",
    "        con_secret = consumer_secret\n",
    "        acc_key = access_key\n",
    "        acc_secret = access_secret \n",
    "        \n",
    "        # Authorize twitter, initialize tweepy\n",
    "        auth = tweepy.OAuthHandler(con_key, con_secret)\n",
    "        auth.set_access_token(acc_key, acc_secret)\n",
    "        api = tweepy.API(auth)\n",
    "\n",
    "        # Collect tweets\n",
    "        tweets = api.user_timeline(screen_name = 'realDonaldTrump', count = 100,  \n",
    "                                   tweet_mode = \"extended\", include_rts = False)\n",
    "\n",
    "        # Create dataframe with tweet text, date, and favorite count\n",
    "        TweetsData = pd.DataFrame([[tweet.created_at, tweet.full_text, tweet.favorite_count] for tweet in tweets], \n",
    "                         columns = ['created_at', 'text', 'favorite_count'])\n",
    "        \n",
    "        ### CLEAN ###\n",
    "        # Format Date\n",
    "        TweetsData['Date'] = TweetsData.created_at.dt.strftime('%Y-%m-%d')\n",
    "        # Change type of Date to integer for merging purposes\n",
    "        TweetsData.Date = TweetsData.Date.astype(str)\n",
    "        \n",
    "        \n",
    "        #########\n",
    "        # MERGE #\n",
    "        #########\n",
    "        # Merge the two dataframes, return \n",
    "        cleaned_df = pd.merge(TweetsData, FollowersData, on='Date', how = 'left')\n",
    "        return(cleaned_df)\n",
    "    \n",
    "    \n",
    "    ### Helper functions for feature engineering ###\n",
    "    # Input: Tweet. Output: Average word length of tweet. \n",
    "    def avg_word_length(self, tweet): \n",
    "        words = tweet.split()\n",
    "        if len(words) > 0:\n",
    "            return(sum(len(word) for word in words) / len(words))\n",
    "        else: \n",
    "            return(0)\n",
    "        \n",
    "    # Input: Tweet. Output: Average term-frequency rate     \n",
    "    def avg_tf_rate(self, tweet):\n",
    "        # For empty tweets (tweets that were likely just urls), mean tf = 0\n",
    "        if len(tweet) == 0:\n",
    "            return(0)\n",
    "        # For actual tweets, calculate it out \n",
    "        else: \n",
    "            return(np.mean(pd.value_counts(tweet.split(' '))/len(tweet)))\n",
    "    \n",
    "    # Input: tweet, idf_df. Output: Average inverse-document-frequency rate \n",
    "    def avg_idf_rate(tweet, idf_df):\n",
    "        # Tokenize the tweet\n",
    "        tokenized_tweet = tweet.split(' ')\n",
    "        # Find the idf per word using the idf_df \n",
    "        idf = idf_df[idf_df.Phrase.isin(tokenized_tweet)].IDF.values\n",
    "        # Safeguard against empty lists - that means idf = 0\n",
    "        if len(idf) == 0:\n",
    "            return(0)\n",
    "        else: \n",
    "            # Otherwise, return the average \n",
    "            return(np.mean(idf))\n",
    "\n",
    "\n",
    "    def EngineerFeatures(self, df): \n",
    "        ###################\n",
    "        # Basic Features #\n",
    "        ##################\n",
    "        \n",
    "        # Create Year, Month, Week, Day, Hour variables \n",
    "        df['Year'] = df.created_at.dt.year\n",
    "        df['Month'] = df.created_at.dt.month\n",
    "        df['Week'] = df.created_at.dt.week\n",
    "        df['Day'] = df.created_at.dt.day\n",
    "        df['Hour'] = df.created_at.dt.hour\n",
    "        \n",
    "        # Add holidays feature from full_holidays.csv - read and merge \n",
    "        fullholidays = pd.read_csv('fullholidays.csv')\n",
    "        df = pd.merge(df, fullholidays, on = 'Date', how = 'left')\n",
    "        # Any non-holiday gets 0 - it will show up as null\n",
    "        df.iloc[df.Holiday.isnull().index, np.where(df.columns.isin(['Holiday']))[0]] = 0\n",
    "        \n",
    "        # Remove urls\n",
    "        df['trump_text'] = df.text.str.replace(r'http\\S+', '')\n",
    "        \n",
    "        # Get word count \n",
    "        df['Word_Count'] = df.trump_text.apply(lambda x: len(str(x).split(' ')))\n",
    "        \n",
    "        # Presence of URLs\n",
    "        # Create column with all 0s\n",
    "        df['Any_urls'] = 0\n",
    "        # Find indices with 0s\n",
    "        indices_with_urls = df.text.str.extractall(r'(http\\S+)').index.get_level_values(0)\n",
    "        # Make those indices 1\n",
    "        df.iloc[indices_with_urls, np.where(df.columns.isin(['Any_urls']))[0]] = 1\n",
    "\n",
    "        # Character count \n",
    "        df['Character_Count'] = df.trump_text.str.len()\n",
    "        \n",
    "        # Average word length\n",
    "        df['avg_word_len'] = df.trump_text.apply(self.avg_word_length)\n",
    "        \n",
    "        # Count number of stopwords \n",
    "        swords = stopwords.words('english')\n",
    "        df['Num_Stopwords'] = df.trump_text.apply(lambda x: \n",
    "                                                  len([word for word in x.split() if word.lower() in swords]))\n",
    "        \n",
    "        # Presence of Hashtags\n",
    "        # Presence \n",
    "        indices_with_hashtags = df.trump_text.str.extractall(r'(#)', re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create an indicator variable, set all to 0 \n",
    "        df['Any_Hashtags'] = 0\n",
    "        # Set indices_with_hashtags to 1\n",
    "        df.iloc[indices_with_hashtags, np.where(df.columns.isin(['Any_Hashtags']))[0]] = 1\n",
    "\n",
    "        # Number of Hashtags\n",
    "        df['Num_Hashtags'] = df.trump_text.apply(lambda x: \n",
    "                                                 len([word for word in x.split() if word.startswith('#')]))\n",
    "        \n",
    "        # Presence of Mentions\n",
    "        indices_with_mentions = df.trump_text.str.extractall(r'(@)', re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Mentions'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_mentions, np.where(df.columns.isin(['Any_Mentions']))[0]] = 1\n",
    "\n",
    "        # Number of Mentions\n",
    "        df['Num_Mentions'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.startswith('#')]))\n",
    "        \n",
    "        # Number of Uppercase words Per tweet \n",
    "        df['Num_Upper'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.isupper()]))\n",
    "\n",
    "        # Presence of Fully Uppercase Tweets \n",
    "        df['Upper'] = df.trump_text.str.isupper().astype(int)\n",
    "\n",
    "        # Number of Exclamation Points\n",
    "        df['Num_Exclaim'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.endswith('!')]))\n",
    "\n",
    "        \n",
    "        # Presence of Clintons \n",
    "        indices_with_clintons = df.trump_text.str.extractall(r'(hillary)|(hillary clinton)|(clinton)|(clintons)|(bill clinton)', \n",
    "                                                     re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Clinton'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_clintons, np.where(df.columns.isin(['Any_Clinton']))[0]] = 1\n",
    "        \n",
    "        \n",
    "        # Presence of Obama\n",
    "        indices_with_obama = df.trump_text.str.extractall(r'(obama)|(barrack)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Obama'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_obama, np.where(df.columns.isin(['Any_Obama']))[0]] = 1\n",
    "\n",
    "        \n",
    "        # Presence of MAGA\n",
    "        indices_with_maga = df.trump_text.str.extractall(r'(MAKE AMERICA GREAT AGAIN)|(MAGA)|(#MAKEAMERICAGREATAGAIN)|(#MAGA)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_MAGA'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_maga, np.where(df.columns.isin(['Any_MAGA']))[0]] = 1       \n",
    "        \n",
    "        \n",
    "        # Presence of Sad/Bad\n",
    "        indices_with_ad = df.trump_text.str.extractall(r'(sad)|(bad)|(sad!)|(bad!)|(sad.)|(bad.)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_ad'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_ad, np.where(df.columns.isin(['Any_ad']))[0]] = 1\n",
    "        \n",
    "        \n",
    "        # Presence of Democrats\n",
    "        indices_with_dems = df.trump_text.str.extractall(r'(democrat)|(democratic)|(democrats)|(dems)|(dem)|(dnc)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Dems'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_dems, np.where(df.columns.isin(['Any_Dems']))[0]] = 1\n",
    "        \n",
    "        \n",
    "        # Presence of Republicans\n",
    "        indices_with_reps = df.trump_text.str.extractall(r'(republican)|(rep)|(gop)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create indicator variable, set all to 0\n",
    "        df['Any_Rep'] = 0\n",
    "        # Set proper indices to 1\n",
    "        df.iloc[indices_with_reps, np.where(df.columns.isin(['Any_Rep']))[0]] = 1\n",
    "\n",
    "\n",
    "        # Presence of fake news \n",
    "        indices_with_media = df.trump_text.str.extractall(r'(fakenews)|(fake news)|(media)|(fake)|(fakenews!)', \n",
    "                                                          re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create column, set all = 0\n",
    "        df['FakeNews'] = 0\n",
    "        # Set those with mentions of fake news = 1\n",
    "        df.iloc[indices_with_media, np.where(df.columns.isin(['FakeNews']))[0]] = 1\n",
    "\n",
    "\n",
    "        # Presence of lies\n",
    "        indices_mentioning_lies = df.trump_text.str.extractall(r'(liar)|(lie)|(false)|(faker)|(con)|(hoax)',\n",
    "                                                              re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create column\n",
    "        df['Any_Lie_Mentions'] = 0\n",
    "        # Make indices_mentioning_lies = 1\n",
    "        df.iloc[indices_mentioning_lies, np.where(df.columns.isin(['Any_Lie_Mentions']))[0]] = 1\n",
    "\n",
    "\n",
    "        # Presence of immigration\n",
    "        indices_mentioning_immigrants = df.trump_text.str.extractall(r'(immigration)|(immigrants)|(border)|(wall)|(buildthewall)|(wall)|(thewall)|(caravan)',\n",
    "                                                              re.IGNORECASE).index.get_level_values(0)\n",
    "        # Create column, set all = 0\n",
    "        df['Any_Immigration'] = 0\n",
    "        # Set those with mentions of fake news = 1\n",
    "        df.iloc[indices_mentioning_immigrants, np.where(df.columns.isin(['Any_Immigration']))[0]] = 1\n",
    "\n",
    "        ####################\n",
    "        # Preprocess text #\n",
    "        ###################\n",
    "        # Lowercase\n",
    "        df.trump_text = df.trump_text.str.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        df.trump_text = df.trump_text.str.replace(pat = '[^\\w\\s]', repl = '')\n",
    "\n",
    "        # Remove stopwords\n",
    "        df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in swords]))\n",
    "\n",
    "        # Remove most commonly occuring words \n",
    "        freq_words = list(pd.Series(' '.join(df['trump_text']).split()).value_counts()[:10].index)\n",
    "        df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in freq_words]))\n",
    "\n",
    "        # Remove all words that were only used once \n",
    "        infreq_words = pd.Series(' '.join(df['trump_text']).split()).value_counts()\n",
    "        infreq_words = infreq_words[infreq_words.values == 1]\n",
    "        infreq_words = list(infreq_words.index)\n",
    "        df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in infreq_words]))\n",
    "\n",
    "        # Lemmatize\n",
    "        df.trump_text = df.trump_text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "        # Algorithmic Text Processing #\n",
    "        ###############################\n",
    "        # TF\n",
    "        df['tf'] = df.trump_text.apply(lambda x: self.avg_tf_rate(x))\n",
    "        \n",
    "        # IDF\n",
    "        # Create tfidf vectorizer\n",
    "        vectorizer = TfidfVectorizer(lowercase=True, analyzer='word', ngram_range=(1,1))\n",
    "        # Load the training text and replace nulls to be empty \n",
    "        training_text = pd.read_csv('training_text.csv')\n",
    "        training_text[training_text.isnull()] = ''\n",
    "        # Append training text to test text \n",
    "        full_text = training_text.trump_text.append(df.trump_text, ignore_index=True)\n",
    "        # Fit the vocabulary \n",
    "        idf_fit = vectorizer.fit(full_text)\n",
    "        # Use the fit to find idf per phrase - save into a dataframe \n",
    "        idf_df = pd.DataFrame({'Phrase': idf_fit.get_feature_names(), 'IDF': idf_fit.idf_})\n",
    "        # Create column\n",
    "        df['idf'] = df.trump_text.apply(lambda x: self.avg_idf_rate(x, idf_df))\n",
    "        \n",
    "        # TF-IDF\n",
    "        df['tfidf'] = df.tf * df.idf\n",
    "        \n",
    "        # Sentiment\n",
    "        df['Sentiment'] = df.trump_text.apply(lambda x: TextBlob(x).sentiment[0])\n",
    "\n",
    "        # Subjectivity \n",
    "        df['Subjectivity'] = df.trump_text.apply(lambda x: TextBlob(x).sentiment[1])\n",
    "        \n",
    "        # Split \n",
    "        X = df.drop(columns=['favorite_count', axis = 1])\n",
    "        y = df.favorite_count\n",
    "        \n",
    "        return(X, y)\n",
    "    \n",
    "    \n",
    "def main(model_file, consumer_key, consumer_secret, access_key, access_secret):    \n",
    "    \n",
    "    # Initialize model\n",
    "    tweet_model = TrumpTwitterPredictions(model_file)\n",
    "\n",
    "    # Gather data\n",
    "    df = tweet_model.CollectData(consumer_key, consumer_secret, access_key, access_secret)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    X, y = tweet_model.EngineerFeatures(df)\n",
    "    \n",
    "    # Predict\n",
    "    pred = tweet_model.predict(X)\n",
    "    \n",
    "    print(pred)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main( *sys.argv[1:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "## Followers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Followers  (change)</th>\n",
       "      <th>Following  (change)</th>\n",
       "      <th>Tweets  (change)</th>\n",
       "      <th>Lists  (change)</th>\n",
       "      <th>Favourites  (change)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>October 27, 2018</td>\n",
       "      <td>55,359,479  (+67,956)</td>\n",
       "      <td>46  (0)</td>\n",
       "      <td>39,431  (+12)</td>\n",
       "      <td>95,133  (+144)</td>\n",
       "      <td>7  (0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>October 26, 2018</td>\n",
       "      <td>55,291,523  (+7,903)</td>\n",
       "      <td>46  (0)</td>\n",
       "      <td>39,419  (+17)</td>\n",
       "      <td>94,989  (+37)</td>\n",
       "      <td>7  (0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>October 25, 2018</td>\n",
       "      <td>55,283,620  (-2,683)</td>\n",
       "      <td>46  (0)</td>\n",
       "      <td>39,402  (+8)</td>\n",
       "      <td>94,952  (+22)</td>\n",
       "      <td>7  (-19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>October 24, 2018</td>\n",
       "      <td>55,286,303  (-9,142)</td>\n",
       "      <td>46  (0)</td>\n",
       "      <td>39,394  (+8)</td>\n",
       "      <td>94,930  (+25)</td>\n",
       "      <td>26  (0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>October 23, 2018</td>\n",
       "      <td>55,295,445  (+11,953)</td>\n",
       "      <td>46  (0)</td>\n",
       "      <td>39,386  (+19)</td>\n",
       "      <td>94,905  (-7)</td>\n",
       "      <td>26  (0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id              Date    Followers  (change) Following  (change)  \\\n",
       "0  1  October 27, 2018  55,359,479  (+67,956)             46  (0)   \n",
       "1  2  October 26, 2018   55,291,523  (+7,903)             46  (0)   \n",
       "2  3  October 25, 2018   55,283,620  (-2,683)             46  (0)   \n",
       "3  4  October 24, 2018   55,286,303  (-9,142)             46  (0)   \n",
       "4  5  October 23, 2018  55,295,445  (+11,953)             46  (0)   \n",
       "\n",
       "  Tweets  (change) Lists  (change) Favourites  (change)  \n",
       "0    39,431  (+12)  95,133  (+144)               7  (0)  \n",
       "1    39,419  (+17)   94,989  (+37)               7  (0)  \n",
       "2     39,402  (+8)   94,952  (+22)             7  (-19)  \n",
       "3     39,394  (+8)   94,930  (+25)              26  (0)  \n",
       "4    39,386  (+19)    94,905  (-7)              26  (0)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and open url \n",
    "url = \"https://www.trackalytics.com/twitter/profile/realdonaldtrump/\"\n",
    "html = urlopen(url)\n",
    "\n",
    "# Create BS object \n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Find table on this page - only one table, index of 0\n",
    "table = soup.findAll('table')[0]\n",
    "\n",
    "# Get column names (exist in second row of table)\n",
    "colnames = [th.getText() for th in table.findAll('tr', limit=2)[1].findAll('th')]\n",
    "\n",
    "# Collect every row with non-header data \n",
    "data_rows = table.findAll('tr')[2:] \n",
    "\n",
    "# For every row, create a list of column data, and append those lists into one large list \n",
    "follower_data = [[td.getText() for td in data_rows[i].findAll('td')] for i in range(len(data_rows))]\n",
    "\n",
    "# Create dataframe \n",
    "df = pd.DataFrame(follower_data, columns=colnames, dtype=str)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-27 21:41:35</td>\n",
       "      <td>...This evil Anti-Semitic attack is an assault...</td>\n",
       "      <td>42794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-27 21:41:35</td>\n",
       "      <td>All of America is in mourning over the mass mu...</td>\n",
       "      <td>51720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-27 19:43:45</td>\n",
       "      <td>As you know, earlier today there was a horrifi...</td>\n",
       "      <td>38561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-27 16:26:23</td>\n",
       "      <td>Events in Pittsburgh are far more devastating ...</td>\n",
       "      <td>65624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-27 15:08:28</td>\n",
       "      <td>Watching the events unfolding in Pittsburgh, P...</td>\n",
       "      <td>69530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_at                                               text  \\\n",
       "0 2018-10-27 21:41:35  ...This evil Anti-Semitic attack is an assault...   \n",
       "1 2018-10-27 21:41:35  All of America is in mourning over the mass mu...   \n",
       "2 2018-10-27 19:43:45  As you know, earlier today there was a horrifi...   \n",
       "3 2018-10-27 16:26:23  Events in Pittsburgh are far more devastating ...   \n",
       "4 2018-10-27 15:08:28  Watching the events unfolding in Pittsburgh, P...   \n",
       "\n",
       "   favorite_count  \n",
       "0           42794  \n",
       "1           51720  \n",
       "2           38561  \n",
       "3           65624  \n",
       "4           69530  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Twitter API credentials\n",
    "consumer_key = \"xK8iqfsNIOJCA4sXDQVxcmFKq\"\n",
    "consumer_secret = \"Lt5dBPX0SUWa9TskVUlydhilBqhu4JcHy13jg1IwoM6V672MkY\"\n",
    "access_key = \"1013962412091850754-L1fvfqKUEaSfrLcRSwmZvhlB79aRcw\"\n",
    "access_secret = \"HN35iHf3wAcFeczjIrDjjjdsmMvBkPRAkSK9VQG7rutht\"\n",
    "\n",
    "# Authorize twitter, initialize tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Collect tweets\n",
    "tweets = api.user_timeline(screen_name = 'realDonaldTrump', count = 200,  tweet_mode = \"extended\", include_rts = False)\n",
    "\n",
    "# Create dataframe with tweet text, date, and favorite count\n",
    "TweetsData = pd.DataFrame([[tweet.created_at, tweet.full_text, tweet.favorite_count] for tweet in tweets], \n",
    "                 columns = ['created_at', 'text', 'favorite_count'])\n",
    "\n",
    "TweetsData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Followers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Follower_Change</th>\n",
       "      <th>Num_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-27</td>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>55291523</td>\n",
       "      <td>7903</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>55283620</td>\n",
       "      <td>2683</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-24</td>\n",
       "      <td>55286303</td>\n",
       "      <td>9142</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>55295445</td>\n",
       "      <td>11953</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Followers  Follower_Change  Num_Tweets\n",
       "0 2018-10-27   55359479            67956          12\n",
       "1 2018-10-26   55291523             7903          17\n",
       "2 2018-10-25   55283620             2683           8\n",
       "3 2018-10-24   55286303             9142           8\n",
       "4 2018-10-23   55295445            11953          19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove first column\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Extract number of followers for each date \n",
    "followers = df['Followers  (change)'].apply(lambda x: x.split(' ')[0])\n",
    "# Remove commas and change to integer\n",
    "followers = followers.str.replace(',', '').astype(int)\n",
    "\n",
    "# Extract change for each date\n",
    "follower_change = df['Followers  (change)'].apply(lambda x: x.split(' ')[2])\n",
    "# Remove parentheses, pluses, minuses, and commas and change to integer\n",
    "follower_change = follower_change.str.replace('\\\\(|\\\\)|\\\\+|\\\\-|\\\\,', '').astype(int)\n",
    "\n",
    "# Extract number of tweets per day\n",
    "tweets_that_day = df['Tweets  (change)'].apply(lambda x: x.split(' ')[2])\n",
    "# Remove parentheses, pluses, minuses, and commas and change to integer\n",
    "tweets_that_day = tweets_that_day.str.replace('\\\\(|\\\\)|\\\\+|\\\\-|\\\\,', '').astype(int)\n",
    "\n",
    "# Make Date into a datetime object\n",
    "date = pd.to_datetime(df.Date)\n",
    "\n",
    "# Place these features into their own dataframe \n",
    "FollowersData = pd.DataFrame({'Date': date, 'Followers': followers, 'Follower_Change': follower_change, \n",
    "                              'Num_Tweets': tweets_that_day})\n",
    "FollowersData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Date that matches format of FollowersData date \n",
    "TweetsData['Date'] = TweetsData.created_at.dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging will only work with Dates as strings\n",
    "TweetsData.Date = TweetsData.Date.astype(str)\n",
    "FollowersData.Date = FollowersData.Date.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185, 4)\n",
      "(1590, 4)\n"
     ]
    }
   ],
   "source": [
    "print(TweetsData.shape)\n",
    "print(FollowersData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge \n",
    "merged_df = pd.merge(TweetsData, FollowersData, on='Date', how = 'left')\n",
    "df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df.created_at.dt.year\n",
    "df['Month'] = df.created_at.dt.month\n",
    "df['Week'] = df.created_at.dt.week\n",
    "df['Day'] = df.created_at.dt.day\n",
    "df['Hour'] = df.created_at.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullholidays = pd.read_csv('fullholidays.csv')\n",
    "\n",
    "# Merge with df\n",
    "df = pd.merge(df, fullholidays, on = 'Date', how = 'left')\n",
    "df.iloc[df.Holiday.isnull().index, np.where(df.columns.isin(['Holiday']))[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls\n",
    "df['trump_text'] = df.text.str.replace(r'http\\S+', '')\n",
    "\n",
    "# Get word count \n",
    "df['Word_Count'] = df.trump_text.apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "# Create column with all 0s\n",
    "df['Any_urls'] = 0\n",
    "# Find indices with 0s\n",
    "indices_with_urls = df.text.str.extractall(r'(http\\S+)').index.get_level_values(0)\n",
    "# Make those indices 1\n",
    "df.iloc[indices_with_urls, np.where(df.columns.isin(['Any_urls']))[0]] = 1\n",
    "\n",
    "# Character count \n",
    "df['Character_Count'] = df.trump_text.str.len()\n",
    "\n",
    "\n",
    "# Define function to generate avg_length\n",
    "def avg_word_length(tweet): \n",
    "    words = tweet.split()\n",
    "    if len(words) > 0:\n",
    "        return(sum(len(word) for word in words) / len(words))\n",
    "    else: \n",
    "        return(0)\n",
    "\n",
    "df['avg_word_len'] = df.trump_text.apply(avg_word_length)\n",
    "\n",
    "swords = stopwords.words('english')\n",
    "df['Num_Stopwords'] = df.trump_text.apply(lambda x: \n",
    "                                          len([word for word in x.split() if word.lower() in swords]))\n",
    "\n",
    "# Presence \n",
    "indices_with_hashtags = df.trump_text.str.extractall(r'(#)', re.IGNORECASE).index.get_level_values(0)\n",
    "# Create an indicator variable, set all to 0 \n",
    "df['Any_Hashtags'] = 0\n",
    "# Set indices_with_hashtags to 1\n",
    "df.iloc[indices_with_hashtags, np.where(df.columns.isin(['Any_Hashtags']))[0]] = 1\n",
    "\n",
    "# Number\n",
    "df['Num_Hashtags'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.startswith('#')]))\n",
    "\n",
    "\n",
    "# Presence\n",
    "indices_with_mentions = df.trump_text.str.extractall(r'(@)', re.IGNORECASE).index.get_level_values(0)\n",
    "# Create indicator variable, set all to 0\n",
    "df['Any_Mentions'] = 0\n",
    "# Set proper indices to 1\n",
    "df.iloc[indices_with_mentions, np.where(df.columns.isin(['Any_Mentions']))[0]] = 1\n",
    "\n",
    "# Number\n",
    "df['Num_Mentions'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.startswith('#')]))\n",
    "\n",
    "# Number a\n",
    "df['Num_Upper'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.isupper()]))\n",
    "\n",
    "# Presence\n",
    "df['Upper'] = df.trump_text.str.isupper().astype(int)\n",
    "\n",
    "# Number of Exclamation Points\n",
    "df['Num_Exclaim'] = df.trump_text.apply(lambda x: len([word for word in x.split() if word.endswith('!')]))\n",
    "\n",
    "# Presence of Clintons\n",
    "indices_with_clintons = df.trump_text.str.extractall(r'(hillary)|(hillary clinton)|(clinton)|(clintons)|(bill clinton)', \n",
    "                                                     re.IGNORECASE).index.get_level_values(0)\n",
    "# Create indicator variable, set all to 0\n",
    "df['Any_Clinton'] = 0\n",
    "# Set proper indices to 1\n",
    "df.iloc[indices_with_clintons, np.where(df.columns.isin(['Any_Clinton']))[0]] = 1\n",
    "\n",
    "# Presence of Obama\n",
    "indices_with_obama = df.trump_text.str.extractall(r'(obama)|(barrack)', \n",
    "                                                     re.IGNORECASE).index.get_level_values(0)\n",
    "# Create indicator variable, set all to 0\n",
    "df['Any_Obama'] = 0\n",
    "# Set proper indices to 1\n",
    "df.iloc[indices_with_obama, np.where(df.columns.isin(['Any_Obama']))[0]] = 1\n",
    "\n",
    "# Presence of MAGA\n",
    "indices_with_maga = df.trump_text.str.extractall(r'(MAKE AMERICA GREAT AGAIN)|(MAGA)|(#MAKEAMERICAGREATAGAIN)|(#MAGA)', \n",
    "                                                     re.IGNORECASE).index.get_level_values(0)\n",
    "# Create indicator variable, set all to 0\n",
    "df['Any_MAGA'] = 0\n",
    "# Set proper indices to 1\n",
    "df.iloc[indices_with_maga, np.where(df.columns.isin(['Any_MAGA']))[0]] = 1\n",
    "\n",
    "# Presence of Sad/Bad\n",
    "indices_with_ad = df.trump_text.str.extractall(r'(sad)|(bad)|(sad!)|(bad!)|(sad.)|(bad.)', \n",
    "                                                     re.IGNORECASE).index.get_level_values(0)\n",
    "# Create indicator variable, set all to 0\n",
    "df['Any_ad'] = 0\n",
    "# Set proper indices to 1\n",
    "df.iloc[indices_with_ad, np.where(df.columns.isin(['Any_ad']))[0]] = 1\n",
    "\n",
    " # Presence of Democrats\n",
    "indices_with_dems = df.trump_text.str.extractall(r'(democrat)|(democratic)|(democrats)|(dems)|(dem)|(dnc)', \n",
    "                                                             re.IGNORECASE).index.get_level_values(0)\n",
    "# Create indicator variable, set all to 0\n",
    "df['Any_Dems'] = 0\n",
    "# Set proper indices to 1\n",
    "df.iloc[indices_with_dems, np.where(df.columns.isin(['Any_Dems']))[0]] = 1\n",
    "\n",
    "\n",
    "# Presence of Republicans\n",
    "indices_with_reps = df.trump_text.str.extractall(r'(republican)|(rep)|(gop)', \n",
    "                                                     re.IGNORECASE).index.get_level_values(0)\n",
    "# Create indicator variable, set all to 0\n",
    "df['Any_Rep'] = 0\n",
    "# Set proper indices to 1\n",
    "df.iloc[indices_with_reps, np.where(df.columns.isin(['Any_Rep']))[0]] = 1\n",
    "\n",
    "\n",
    "# Presence of fake news \n",
    "indices_with_media = df.trump_text.str.extractall(r'(fakenews)|(fake news)|(media)|(fake)|(fakenews!)', \n",
    "                                                  re.IGNORECASE).index.get_level_values(0)\n",
    "# Create column, set all = 0\n",
    "df['FakeNews'] = 0\n",
    "# Set those with mentions of fake news = 1\n",
    "df.iloc[indices_with_media, np.where(df.columns.isin(['FakeNews']))[0]] = 1\n",
    "\n",
    "\n",
    "# Presence of lies\n",
    "indices_mentioning_lies = df.trump_text.str.extractall(r'(liar)|(lie)|(false)|(faker)|(con)|(hoax)',\n",
    "                                                      re.IGNORECASE).index.get_level_values(0)\n",
    "# Create column\n",
    "df['Any_Lie_Mentions'] = 0\n",
    "# Make indices_mentioning_lies = 1\n",
    "df.iloc[indices_mentioning_lies, np.where(df.columns.isin(['Any_Lie_Mentions']))[0]] = 1\n",
    "\n",
    "\n",
    "# Presence of immigration\n",
    "indices_mentioning_immigrants = df.trump_text.str.extractall(r'(immigration)|(immigrants)|(border)|(wall)|(buildthewall)|(wall)|(thewall)|(caravan)',\n",
    "                                                      re.IGNORECASE).index.get_level_values(0)\n",
    "# Create column, set all = 0\n",
    "df['Any_Immigration'] = 0\n",
    "# Set those with mentions of fake news = 1\n",
    "df.iloc[indices_mentioning_immigrants, np.where(df.columns.isin(['Any_Immigration']))[0]] = 1\n",
    "\n",
    "\n",
    "# Lowercase\n",
    "df.trump_text = df.trump_text.str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "df.trump_text = df.trump_text.str.replace(pat = '[^\\w\\s]', repl = '')\n",
    "\n",
    "# Remove stopwords\n",
    "df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in swords]))\n",
    "\n",
    "# Remove most commonly occuring words \n",
    "freq_words = list(pd.Series(' '.join(df['trump_text']).split()).value_counts()[:10].index)\n",
    "df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in freq_words]))\n",
    "\n",
    "# Remove all words that were only used once \n",
    "infreq_words = pd.Series(' '.join(df['trump_text']).split()).value_counts()\n",
    "infreq_words = infreq_words[infreq_words.values == 1]\n",
    "infreq_words = list(infreq_words.index)\n",
    "df.trump_text = df.trump_text.apply(lambda x: ' '.join([word for word in x.split() if word not in infreq_words]))\n",
    "\n",
    "# Lemmatize\n",
    "df.trump_text = df.trump_text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "def avg_tf_rate(tweet):\n",
    "    # For empty tweets (tweets that were likely just urls), mean tf = 0\n",
    "    if len(tweet) == 0:\n",
    "        return(0)\n",
    "    # For actual tweets, calculate it out \n",
    "    else: \n",
    "        return(np.mean(pd.value_counts(tweet.split(' '))/len(tweet)))\n",
    "    \n",
    "# Create tf column \n",
    "df['tf'] = df.trump_text.apply(lambda x: avg_tf_rate(x))\n",
    "\n",
    " # IDF\n",
    "# Create tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True, analyzer='word', ngram_range=(1,1))\n",
    "# Load the training text and replace nulls to be empty \n",
    "training_text = pd.read_csv('training_text.csv')\n",
    "training_text[training_text.isnull()] = ''\n",
    "# Append training text to test text \n",
    "full_text = training_text.trump_text.append(df.trump_text, ignore_index=True)\n",
    "# Fit the vocabulary \n",
    "idf_fit = vectorizer.fit(full_text)\n",
    "# Use the fit to find idf per phrase - save into a dataframe \n",
    "idf_df = pd.DataFrame({'Phrase': idf_fit.get_feature_names(), 'IDF': idf_fit.idf_})\n",
    "# Create column\n",
    "df['idf'] = df.trump_text.apply(lambda x: avg_idf_rate(x, idf_df))\n",
    "        \n",
    "# TF-IDF\n",
    "df['tfidf'] = df.tf * df.idf \n",
    "\n",
    "# Sentiment\n",
    "df['Sentiment'] = df.trump_text.apply(lambda x: TextBlob(x).sentiment[0])\n",
    "\n",
    "# Subjectivity \n",
    "df['Subjectivity'] = df.trump_text.apply(lambda x: TextBlob(x).sentiment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['favorite_count', 'Followers', 'Follower_Change', 'Num_Tweets', 'Year',\n",
       "       'Month', 'Week', 'Day', 'Hour', 'Holiday', 'Word_Count', 'Any_urls',\n",
       "       'Character_Count', 'avg_word_len', 'Num_Stopwords', 'Any_Hashtags',\n",
       "       'Num_Hashtags', 'Any_Mentions', 'Num_Mentions', 'Num_Upper', 'Upper',\n",
       "       'Num_Exclaim', 'Any_Clinton', 'Any_Obama', 'Any_MAGA', 'Any_ad',\n",
       "       'Any_Dems', 'Any_Rep', 'FakeNews', 'Any_Lie_Mentions',\n",
       "       'Any_Immigration', 'tf', 'idf', 'tfidf', 'Sentiment', 'Subjectivity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the columns\n",
    "df.drop(columns = ['created_at', 'text', 'Date', 'trump_text'], inplace = True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Follower_Change</th>\n",
       "      <th>Num_Tweets</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>Any_Dems</th>\n",
       "      <th>Any_Rep</th>\n",
       "      <th>FakeNews</th>\n",
       "      <th>Any_Lie_Mentions</th>\n",
       "      <th>Any_Immigration</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42794</td>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>5.581636</td>\n",
       "      <td>0.118758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51720</td>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>6.886043</td>\n",
       "      <td>0.081977</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38561</td>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>6.964586</td>\n",
       "      <td>0.058038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65624</td>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>6.379303</td>\n",
       "      <td>0.053608</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69530</td>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>6.038696</td>\n",
       "      <td>0.083871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_count  Followers  Follower_Change  Num_Tweets  Year  Month  Week  \\\n",
       "0           42794   55359479            67956          12  2018     10    43   \n",
       "1           51720   55359479            67956          12  2018     10    43   \n",
       "2           38561   55359479            67956          12  2018     10    43   \n",
       "3           65624   55359479            67956          12  2018     10    43   \n",
       "4           69530   55359479            67956          12  2018     10    43   \n",
       "\n",
       "   Day  Hour  Holiday      ...       Any_Dems  Any_Rep  FakeNews  \\\n",
       "0   27    21      0.0      ...              0        0         0   \n",
       "1   27    21      0.0      ...              0        0         0   \n",
       "2   27    19      0.0      ...              0        0         0   \n",
       "3   27    16      0.0      ...              0        0         1   \n",
       "4   27    15      0.0      ...              0        0         0   \n",
       "\n",
       "   Any_Lie_Mentions  Any_Immigration        tf       idf     tfidf  Sentiment  \\\n",
       "0                 1                0  0.021277  5.581636  0.118758   0.000000   \n",
       "1                 0                0  0.011905  6.886043  0.081977   0.375000   \n",
       "2                 1                0  0.008333  6.964586  0.058038   0.000000   \n",
       "3                 0                0  0.008403  6.379303  0.053608   0.033333   \n",
       "4                 0                0  0.013889  6.038696  0.083871   0.000000   \n",
       "\n",
       "   Subjectivity  \n",
       "0      0.000000  \n",
       "1      0.450000  \n",
       "2      0.166667  \n",
       "3      0.475000  \n",
       "4      0.000000  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['favorite_count'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Followers</th>\n",
       "      <th>Follower_Change</th>\n",
       "      <th>Num_Tweets</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>...</th>\n",
       "      <th>Any_Dems</th>\n",
       "      <th>Any_Rep</th>\n",
       "      <th>FakeNews</th>\n",
       "      <th>Any_Lie_Mentions</th>\n",
       "      <th>Any_Immigration</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>5.581636</td>\n",
       "      <td>0.118758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>6.886043</td>\n",
       "      <td>0.081977</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>6.964586</td>\n",
       "      <td>0.058038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>6.379303</td>\n",
       "      <td>0.053608</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55359479</td>\n",
       "      <td>67956</td>\n",
       "      <td>12</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>6.038696</td>\n",
       "      <td>0.083871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Followers  Follower_Change  Num_Tweets  Year  Month  Week  Day  Hour  \\\n",
       "0   55359479            67956          12  2018     10    43   27    21   \n",
       "1   55359479            67956          12  2018     10    43   27    21   \n",
       "2   55359479            67956          12  2018     10    43   27    19   \n",
       "3   55359479            67956          12  2018     10    43   27    16   \n",
       "4   55359479            67956          12  2018     10    43   27    15   \n",
       "\n",
       "   Holiday  Word_Count      ...       Any_Dems  Any_Rep  FakeNews  \\\n",
       "0      0.0          32      ...              0        0         0   \n",
       "1      0.0          45      ...              0        0         0   \n",
       "2      0.0          42      ...              0        0         0   \n",
       "3      0.0          48      ...              0        0         1   \n",
       "4      0.0          31      ...              0        0         0   \n",
       "\n",
       "   Any_Lie_Mentions  Any_Immigration        tf       idf     tfidf  Sentiment  \\\n",
       "0                 1                0  0.021277  5.581636  0.118758   0.000000   \n",
       "1                 0                0  0.011905  6.886043  0.081977   0.375000   \n",
       "2                 1                0  0.008333  6.964586  0.058038   0.000000   \n",
       "3                 0                0  0.008403  6.379303  0.053608   0.033333   \n",
       "4                 0                0  0.013889  6.038696  0.083871   0.000000   \n",
       "\n",
       "   Subjectivity  \n",
       "0      0.000000  \n",
       "1      0.450000  \n",
       "2      0.166667  \n",
       "3      0.475000  \n",
       "4      0.000000  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/SamadPatel/Documents/GitHub/Trump-Twitter-Predictions/Data/xgb_model', 'rb') as f:\n",
    "     mod = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
